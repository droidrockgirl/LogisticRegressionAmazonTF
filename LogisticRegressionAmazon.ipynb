{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  2477 \n",
      "\n",
      " === Reading and encoding data... === \n",
      "\n",
      "Progress:  5000\n",
      "Progress:  10000\n",
      "Progress:  15000\n",
      "Progress:  20000\n",
      "Progress:  25000\n",
      "Progress:  30000\n",
      "Progress:  35000\n",
      "\n",
      "Dataset size:  37126 \n",
      "\n",
      "Training dataset size:  29700 \n",
      "\n",
      "\n",
      "Initializing session...\n",
      "\n",
      "\n",
      " =========== STEP  0  =========== \n",
      "\n",
      "Step:  0    Cost:   65600.0\n",
      "Step:  0    Diff:  65600.0\n",
      "\n",
      " =========== STEP  1  =========== \n",
      "\n",
      "Step:  1    Cost:   65570.5\n",
      "Step:  1    Diff:  29.5156\n",
      "\n",
      " =========== STEP  2  =========== \n",
      "\n",
      "Step:  2    Cost:   65581.6\n",
      "Step:  2    Diff:  11.0781\n",
      "\n",
      " =========== STEP  3  =========== \n",
      "\n",
      "Step:  3    Cost:   65588.0\n",
      "Step:  3    Diff:  6.47656\n",
      "\n",
      " =========== STEP  4  =========== \n",
      "\n",
      "Step:  4    Cost:   65595.8\n",
      "Step:  4    Diff:  7.72656\n",
      "\n",
      " =========== STEP  5  =========== \n",
      "\n",
      "Step:  5    Cost:   65566.1\n",
      "Step:  5    Diff:  29.7031\n",
      "\n",
      " =========== STEP  6  =========== \n",
      "\n",
      "Step:  6    Cost:   65593.8\n",
      "Step:  6    Diff:  27.7188\n",
      "\n",
      " =========== STEP  7  =========== \n",
      "\n",
      "Step:  7    Cost:   65588.7\n",
      "Step:  7    Diff:  5.08594\n",
      "\n",
      " =========== STEP  8  =========== \n",
      "\n",
      "Step:  8    Cost:   65594.6\n",
      "Step:  8    Diff:  5.92188\n",
      "\n",
      " =========== STEP  9  =========== \n",
      "\n",
      "Step:  9    Cost:   65607.1\n",
      "Step:  9    Diff:  12.5078\n"
     ]
    }
   ],
   "source": [
    "import gzip, math, numpy, re, string, random, os.path\n",
    "import tensorflow as tf\n",
    "\n",
    "path = \"reviews_Amazon_Instant_Video_5.json.gz\"\n",
    "vocab_path = \"AFINN-111.txt\"\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "\n",
    "def parse_review_dataset(path):\n",
    "  g = gzip.open(path, 'r')\n",
    "  for l in g:\n",
    "    yield eval(l)\n",
    "    \n",
    "def parse_vocabulary(path):\n",
    "    vocabulary_dict = {}\n",
    "    for line in open(path, 'r').readlines():\n",
    "        (word, measure) = line.strip().split('\\t')\n",
    "        \n",
    "        # Measure is an int in range [-5, 5]\n",
    "        vocabulary_dict[word] = int(measure)\n",
    "    return vocabulary_dict\n",
    "\n",
    "def split_strip_punctuation(text):\n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    no_punc_text = regex.sub('', text.lower())\n",
    "    words_only = no_punc_text.split()\n",
    "    #list of words in review\n",
    "    #print(words_only)\n",
    "    return words_only\n",
    "\n",
    "word_in_text = lambda word, text: 1.0 if word in text else 0.0\n",
    "\n",
    "def encode_review_text(text):\n",
    "    word_list = split_strip_punctuation(text)\n",
    "    encoded_vector = [word_in_text(vocab_word, text) for vocab_word in vocabulary_dict]\n",
    "    return encoded_vector\n",
    "\n",
    "vocabulary_dict = parse_vocabulary(vocab_path)\n",
    "print(\"Vocabulary size: \", len(vocabulary_dict), \"\\n\")\n",
    "\n",
    "data = []\n",
    "\n",
    "print(\" === Reading and encoding data... === \\n\")\n",
    "\n",
    "k = 0\n",
    "for review in parse_review_dataset(path):\n",
    "    review_item = []\n",
    "    review_item.append(encode_review_text(review['summary'] + \" \" + review['reviewText']))\n",
    "    review_item.append(int(review['overall']))\n",
    "    data.append(review_item)\n",
    "    k += 1\n",
    "    if k % 5000 == 0:\n",
    "        print(\"Progress: \", k)\n",
    "\n",
    "data_size = len(data)\n",
    "training_data_size = math.floor(data_size * 0.8)\n",
    "\n",
    "print(\"\\nDataset size: \", len(data), \"\\n\")\n",
    "print(\"Training dataset size: \", training_data_size, \"\\n\")\n",
    "\n",
    "\n",
    "num_features = len(vocabulary_dict)\n",
    "num_samples = training_data_size\n",
    "num_classes = 5\n",
    "   \n",
    "# Features = Words\n",
    "# Feature Matrix\n",
    "\n",
    "X = tf.placeholder(tf.float32, [num_samples, num_features])\n",
    "Y = tf.placeholder(tf.int32, [num_samples])\n",
    "\n",
    "Y_one_hot = tf.one_hot(Y, depth=num_classes)\n",
    "\n",
    "weights = tf.Variable(tf.random_normal([num_features, num_classes],\n",
    "                                       mean=0,\n",
    "                                       stddev=0.01,\n",
    "                                       name=\"weights\"))\n",
    "\n",
    "bias = tf.Variable(tf.zeros([1,num_classes], name=\"bias\"))\n",
    "\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "apply_weights_op = tf.matmul(X, weights, name=\"apply_weights\")\n",
    "add_bias_op = tf.add(apply_weights_op, bias, name=\"add_bias\") \n",
    "activation_op = tf.nn.sigmoid(add_bias_op, name=\"activation\")\n",
    "cost_op = tf.nn.l2_loss(tf.nn.sigmoid_cross_entropy_with_logits(logits=activation_op, labels=Y_one_hot), name=\"cost\")\n",
    "    \n",
    "training_op = tf.train.AdamOptimizer(learning_rate).minimize(cost_op)\n",
    "\n",
    "cost = 0\n",
    "diff = 1\n",
    "num_epochs = 10 #100\n",
    "\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if os.path.isfile(\"LogisticRegressionAmazon.ckpt\"):\n",
    "        saver.restore(sess, \"LogisticRegressionAmazon.ckpt\")\n",
    "    else:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print(\"\\nInitializing session...\\n\")\n",
    "\n",
    "    for i in range(num_epochs):\n",
    "        #if i > 1 and diff < .0001:\n",
    "        #    print(\"\\nConvergence. Diff: \"%diff)\n",
    "        #    break\n",
    "        #else:\n",
    "            print(\"\\n =========== STEP \", i, \" =========== \\n\")\n",
    "            \n",
    "            train_data = random.sample(data, training_data_size)\n",
    "            train_data_X, train_data_Y = zip(*random.sample(data, training_data_size))\n",
    "            #print(train_data_X[0], \"  ===>  Rating: \", train_data_Y[0], \"\\n\")\n",
    "            \n",
    "            newCost = sess.run(cost_op, feed_dict={X: train_data_X, Y: train_data_Y})\n",
    "            diff = abs(newCost - cost)\n",
    "            cost = newCost\n",
    "\n",
    "            print(\"Step: \", i, \"   Cost:  \", newCost)\n",
    "            print(\"Step: \", i, \"   Diff: \", diff)\n",
    "\n",
    "            saver = tf.train.Saver()\n",
    "            saver.save(sess, \"LogisticRegressionAmazon.ckpt\")\n",
    "            \n",
    "            biasSummary = tf.summary.histogram(\"biases\", bias.eval(session=sess))\n",
    "    \n",
    "sess.close()\n",
    "\n",
    "# How to get the unsampled data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
